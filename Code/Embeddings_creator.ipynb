{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code allows to create the embeddings, and it contains code from Muennighoff, N. (2022). Sgpt: Gpt sentence embeddings for semantic search. \n",
    "Accessible at https://arxiv.org/abs/2202.08904. \n",
    "The code has been modiefied and adapted to fulfill the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "model_name = 'bigscience/bloom-560m'\n",
    "#to create the embeddings for GPT-Neo, comment the line above this and uncomment the one under this.\n",
    "#model_name = 'EleutherAI/gpt-neo-1.3B'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "texts = pd.read_csv(\"combined-set.csv\")[\"selftext\"].to_list()\n",
    "#sample_size = 10\n",
    "#texts = texts[:sample_size]\n",
    "\n",
    "# Tokenize input texts\n",
    "\n",
    "\n",
    "batch_tokens = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "batchsize = 1\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for i in tqdm(range(0, len(texts), batchsize)):\n",
    "    batch = texts[i:i+batchsize]\n",
    "    batch_tokens = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Get the embeddings\n",
    "    with torch.no_grad():\n",
    "        # Get hidden state of shape [bs, seq_len, hid_dim]\n",
    "        last_hidden_state = model(**batch_tokens, output_hidden_states=True, return_dict=True).last_hidden_state\n",
    "\n",
    "    # Get weights of shape [bs, seq_len, hid_dim]\n",
    "    weights = (\n",
    "        torch.arange(start=1, end=last_hidden_state.shape[1] + 1)\n",
    "        .unsqueeze(0)\n",
    "        .unsqueeze(-1)\n",
    "        .expand(last_hidden_state.size())\n",
    "        .float().to(last_hidden_state.device)\n",
    "    )\n",
    "\n",
    "    # Get attn mask of shape [bs, seq_len, hid_dim]\n",
    "    input_mask_expanded = (\n",
    "        batch_tokens[\"attention_mask\"]\n",
    "        .unsqueeze(-1)\n",
    "        .expand(last_hidden_state.size())\n",
    "        .float()\n",
    "    )\n",
    "\n",
    "    # Perform weighted mean pooling across seq_len: bs, seq_len, hidden_dim -> bs, hidden_dim\n",
    "    sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded * weights, dim=1)\n",
    "    sum_mask = torch.sum(input_mask_expanded * weights, dim=1)\n",
    "\n",
    "    embeddings = sum_embeddings / sum_mask\n",
    "    embeddings= np.array(embeddings)\n",
    "    outputs.extend(embeddings)\n",
    "\n",
    "final_embeddings = pd.DataFrame(data=outputs)\n",
    "\n",
    "print(final_embeddings)\n",
    "final_embeddings.to_csv(\"BLOOM_embedded.csv\")\n",
    "#final_embeddings.to_csv(\"GPTNeo_embedded.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
