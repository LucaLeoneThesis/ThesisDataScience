{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable a specific warning\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import pickle\n",
    "from typing import List, Text, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "#To change the model run, change the name of CURRENT_RUN into one of the models contained in dictionary MODELS.\n",
    "CURRENT_RUN = \"BLOOM\"\n",
    "\n",
    "\n",
    "MODELS = {\n",
    "    \"BLOOM\": \"bigscience/bloom-560m\",\n",
    "    \"GPTNeo\": \"EleutherAI/gpt-neo-1.3B\"\n",
    "}\n",
    "\n",
    "TEST_RUN = False\n",
    "MAX_TEXT_LENGTH = 10_000\n",
    "MIN_TEXT_LENGTH = 100\n",
    "\n",
    "\n",
    "# Load text\n",
    "def load_explanation_dataset() -> List[Text]:\n",
    "    \"\"\"Function to extract all texts from the input data CSV-files\n",
    "\n",
    "    Returns:\n",
    "        (List[text]): A list of self_texts\n",
    "    \"\"\"\n",
    "    self_texts = pd.read_csv(\"input_data/combined-set.csv\")\n",
    "    self_texts = self_texts[self_texts[\"selftext\"].str.len() < MAX_TEXT_LENGTH]\n",
    "    self_texts = self_texts[self_texts[\"selftext\"].str.len() > MIN_TEXT_LENGTH]\n",
    "    self_texts = self_texts[\"selftext\"]\n",
    "\n",
    "    # Sample a subset of the training data\n",
    "    sample_size = 200\n",
    "    if TEST_RUN:\n",
    "        sample_size = 10\n",
    "    self_texts = self_texts.sample(sample_size)\n",
    "\n",
    "    # Convert to List[Text] and order by length to parse slowes examples first\n",
    "    self_texts = self_texts.to_list()\n",
    "    self_texts = sorted(self_texts, key=len, reverse=True)\n",
    "\n",
    "    return self_texts\n",
    "\n",
    "\n",
    "\n",
    "def load_prediction_model(model_name: Text) -> tf.keras.Model:\n",
    "    \"\"\"Loads a pre-trained prediction model from a folder\n",
    "\n",
    "    Args:\n",
    "        model_name (Text): name of the model\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: a Keras Model\n",
    "    \"\"\"\n",
    "    model_folder = f\"saved_models/{model_name}_aftercorrection/\"\n",
    "    model = tf.keras.models.load_model(model_folder)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_transformer_models(\n",
    "    model_name: Text,\n",
    ") -> Union[transformers.PreTrainedTokenizer, transformers.PreTrainedModel]:\n",
    "    \"\"\"Load the pretrained embedding model and associated tokenizer.\n",
    "\n",
    "    Args:\n",
    "        model_name (Text): name of the base model.\n",
    "\n",
    "    Returns:\n",
    "        Union[transformers.PreTrainedTokenizer, transformers.PreTrainedModel]:\n",
    "            pretrained tokenizer and model from huggingface\n",
    "    \"\"\"\n",
    "    transformer_model = MODELS[model_name]\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(transformer_model)\n",
    "\n",
    "    # Neo has no padding token included\n",
    "    if \"neo\" in model_name.lower():\n",
    "        # remediate the lack of provided padding token.\n",
    "        # Should be filtered by the attention mask.\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    embedding_model = transformers.AutoModel.from_pretrained(transformer_model)\n",
    "\n",
    "    return (tokenizer, embedding_model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code allows to create the embeddings and calculate SHAP values, and it contains code from Muennighoff, N. (2022). Sgpt: Gpt sentence embeddings for semantic search. \n",
    "Accessible at https://arxiv.org/abs/2202.08904. \n",
    "Their code has been modiefied and adapted to fulfill the task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(\n",
    "    last_hidden_state: torch.Tensor, attention_mask: torch.Tensor\n",
    ") -> np.array:\n",
    "    \"\"\"Computes the embeddings based on the last hidden state of a model\n",
    "\n",
    "    This is an implementation of Bi-Encoder symmetric search following this\n",
    "    paper: https://arxiv.org/pdf/2202.08904.pdf\n",
    "    Args:\n",
    "        last_hidden_state (torch.Tensor): last hidden staten of the embedding model.\n",
    "                has the shape: [batch_size, sequence_length, hidden_dimmension]\n",
    "        attention_mask (torch.Tensor): attention mask for the sequence.\n",
    "\n",
    "    Returns:\n",
    "        np.array: array of the embeddings of shape [batch_size, hidden_dimension]\n",
    "    \"\"\"\n",
    "    # Get weights of shape [bs, seq_len, hid_dim]\n",
    "    hidden_state_shape = last_hidden_state.shape\n",
    "    sequence_length = hidden_state_shape[1]\n",
    "\n",
    "    # Weights will increase linearly for each point in the sequence.\n",
    "    # For a hidden_sate with batch_size=1, sequence_length=2, hidden_dim=3,\n",
    "    # it will look like this:\n",
    "    # weights = torch.Tensor(\n",
    "    #   [\n",
    "    #       [[1., 1., 1.],\n",
    "    #       [2., 2., 2.]]\n",
    "    #   ]\n",
    "    # )\n",
    "    weights = (\n",
    "        torch.arange(start=1, end=sequence_length + 1)\n",
    "        .unsqueeze(0)\n",
    "        .unsqueeze(-1)\n",
    "        .expand(hidden_state_shape)\n",
    "        .float()\n",
    "        .to(last_hidden_state.device)\n",
    "    )\n",
    "\n",
    "    # Get attention mask of shape [bs, seq_len, hid_dim]\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(hidden_state_shape).float()\n",
    "    )\n",
    "\n",
    "    # Perform weighted mean pooling across seq_len: bs, seq_len, hidden_dim -> bs, hidden_dim\n",
    "    sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded * weights, dim=1)\n",
    "    sum_mask = torch.sum(input_mask_expanded * weights, dim=1)\n",
    "\n",
    "    embeddings = sum_embeddings / sum_mask\n",
    "    embeddings = np.array(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def load_custom_predict_fn(model_name: Text, return_tokenizer: bool = True):\n",
    "    \"\"\"create a predict function with the signature np.array[samples] -> np.array[probabilities]\n",
    "\n",
    "    Args:\n",
    "        model_name (Text): name of the model for which to build a prediction function.\n",
    "        return_tokenizer (bool, optional): whether to return both the prediction\n",
    "            function and the tokenizer. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: contains either (prediction_function) or (prediction_function, tokenizer)\n",
    "    \"\"\"\n",
    "    tokenizer, embedding_model = load_transformer_models(model_name)\n",
    "    prediction_model = load_prediction_model(model_name)\n",
    "\n",
    "    # define a prediction function\n",
    "    def custom_predict_fn(samples: np.array) -> np.array:\n",
    "        \"\"\"Predict function that accepts an np.array and return the prediction.\n",
    "\n",
    "        Args:\n",
    "            samples (np.array[str]): np.array of strings of arbitrary batch size,\n",
    "                passed by the explainer object\n",
    "\n",
    "        Returns:\n",
    "            np.array[float]: np.array of floats representing the assigned confidence\n",
    "                for the positive class. the batch_size (np.array.shape[0] must match with `x`)\n",
    "        \"\"\"\n",
    "        # BUG: Hugging face only accepts List[Str] as input. However, SHAP Explainer passes\n",
    "        # a np.array[str]. So we must manually convert the array to a string before passing\n",
    "        # it to the tokenizer.\n",
    "        samples_as_list = list(samples)\n",
    "        batch_tokens = tokenizer(\n",
    "            samples_as_list,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_TEXT_LENGTH,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get hidden state of shape [bs, seq_len, hid_dim]\n",
    "            last_hidden_state = embedding_model(\n",
    "                **batch_tokens, output_hidden_states=True\n",
    "            ).last_hidden_state\n",
    "\n",
    "        embeddings = compute_embeddings(\n",
    "            last_hidden_state, batch_tokens[\"attention_mask\"]\n",
    "        )\n",
    "\n",
    "        y_pred = prediction_model.predict(embeddings, verbose=0)\n",
    "        y_pred = y_pred.reshape(-1)\n",
    "\n",
    "        assert (\n",
    "            samples.shape[0] == y_pred.shape[0]\n",
    "        ), \"The code should return as many predictions as \"\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    if return_tokenizer:\n",
    "        output = (custom_predict_fn, tokenizer)\n",
    "    else:\n",
    "        output = custom_predict_fn\n",
    "    return output\n",
    "\n",
    "\n",
    "def save_results(results) -> None:\n",
    "    \"\"\"Save results of a Shapley Explainer to a file\n",
    "\n",
    "    Args:\n",
    "        results (shap.ExplainerObject): results of the explainer run.\n",
    "    \"\"\"\n",
    "    filename = (\n",
    "        f\"explanations/{'test_' if TEST_RUN else ''}{CURRENT_RUN}_explanations.pickle\"\n",
    "    )\n",
    "\n",
    "    # Pickle the object to a file\n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump(results, file)\n",
    "\n",
    "\n",
    "def get_embeddings():\n",
    "    \"\"\"Load the relevant objects, and calculate the SHAP values\n",
    "    then save the results.\n",
    "\n",
    "    Main function of the script.\n",
    "    \"\"\"\n",
    "    prediction_fn, tokenizer = load_custom_predict_fn(\n",
    "        model_name=CURRENT_RUN, return_tokenizer=True\n",
    "    )\n",
    "\n",
    "    # build an explainer using a token masker\n",
    "    explainer = shap.Explainer(prediction_fn, tokenizer)\n",
    "\n",
    "    # load the dataset for our model\n",
    "    text_dataset = load_explanation_dataset()\n",
    "\n",
    "    shap_values = explainer(\n",
    "        text_dataset,\n",
    "        fixed_context=1,\n",
    "    )\n",
    "\n",
    "    save_results(shap_values)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_embeddings()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
